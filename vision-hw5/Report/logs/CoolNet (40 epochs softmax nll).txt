[1,  2000] loss: 2.165
[1,  4000] loss: 1.886
[1,  6000] loss: 1.764
[1,  8000] loss: 1.672
[1, 10000] loss: 1.593
[1, 12000] loss: 1.545
Final Summary:   loss: 1.760
Train Accuracy of the network: 46 %
Test Accuracy of the network: 46 %
[2,  2000] loss: 1.492
[2,  4000] loss: 1.476
[2,  6000] loss: 1.444
[2,  8000] loss: 1.419
[2, 10000] loss: 1.387
[2, 12000] loss: 1.350
Final Summary:   loss: 1.427
[3,  2000] loss: 1.343
[3,  4000] loss: 1.320
[3,  6000] loss: 1.310
[3,  8000] loss: 1.283
[3, 10000] loss: 1.300
[3, 12000] loss: 1.267
Final Summary:   loss: 1.303
[4,  2000] loss: 1.236
[4,  4000] loss: 1.228
[4,  6000] loss: 1.223
[4,  8000] loss: 1.203
[4, 10000] loss: 1.189
[4, 12000] loss: 1.177
Final Summary:   loss: 1.207
[5,  2000] loss: 1.144
[5,  4000] loss: 1.134
[5,  6000] loss: 1.149
[5,  8000] loss: 1.130
[5, 10000] loss: 1.128
[5, 12000] loss: 1.107
Final Summary:   loss: 1.132
[6,  2000] loss: 1.087
[6,  4000] loss: 1.098
[6,  6000] loss: 1.057
[6,  8000] loss: 1.079
[6, 10000] loss: 1.052
[6, 12000] loss: 1.070
Final Summary:   loss: 1.073
[7,  2000] loss: 1.046
[7,  4000] loss: 1.037
[7,  6000] loss: 1.014
[7,  8000] loss: 1.027
[7, 10000] loss: 1.016
[7, 12000] loss: 0.999
Final Summary:   loss: 1.021
[8,  2000] loss: 0.971
[8,  4000] loss: 0.975
[8,  6000] loss: 1.003
[8,  8000] loss: 0.995
[8, 10000] loss: 0.988
[8, 12000] loss: 0.996
Final Summary:   loss: 0.986
[9,  2000] loss: 0.952
[9,  4000] loss: 0.968
[9,  6000] loss: 0.934
[9,  8000] loss: 0.964
[9, 10000] loss: 0.951
[9, 12000] loss: 0.951
Final Summary:   loss: 0.953
[10,  2000] loss: 0.934
[10,  4000] loss: 0.947
[10,  6000] loss: 0.917
[10,  8000] loss: 0.923
[10, 10000] loss: 0.933
[10, 12000] loss: 0.917
Final Summary:   loss: 0.929
[11,  2000] loss: 0.890
[11,  4000] loss: 0.909
[11,  6000] loss: 0.909
[11,  8000] loss: 0.879
[11, 10000] loss: 0.915
[11, 12000] loss: 0.933
Final Summary:   loss: 0.907
[12,  2000] loss: 0.892
[12,  4000] loss: 0.886
[12,  6000] loss: 0.880
[12,  8000] loss: 0.871
[12, 10000] loss: 0.899
[12, 12000] loss: 0.903
Final Summary:   loss: 0.890
[13,  2000] loss: 0.869
[13,  4000] loss: 0.875
[13,  6000] loss: 0.874
[13,  8000] loss: 0.879
[13, 10000] loss: 0.870
[13, 12000] loss: 0.888
Final Summary:   loss: 0.876
[14,  2000] loss: 0.858
[14,  4000] loss: 0.848
[14,  6000] loss: 0.846
[14,  8000] loss: 0.867
[14, 10000] loss: 0.853
[14, 12000] loss: 0.882
Final Summary:   loss: 0.860
[15,  2000] loss: 0.859
[15,  4000] loss: 0.826
[15,  6000] loss: 0.844
[15,  8000] loss: 0.857
[15, 10000] loss: 0.861
[15, 12000] loss: 0.866
Final Summary:   loss: 0.851
[16,  2000] loss: 0.845
[16,  4000] loss: 0.828
[16,  6000] loss: 0.833
[16,  8000] loss: 0.844
[16, 10000] loss: 0.821
[16, 12000] loss: 0.852
Final Summary:   loss: 0.839
[17,  2000] loss: 0.835
[17,  4000] loss: 0.828
[17,  6000] loss: 0.806
[17,  8000] loss: 0.834
[17, 10000] loss: 0.833
[17, 12000] loss: 0.831
Final Summary:   loss: 0.828
[18,  2000] loss: 0.802
[18,  4000] loss: 0.825
[18,  6000] loss: 0.807
[18,  8000] loss: 0.821
[18, 10000] loss: 0.839
[18, 12000] loss: 0.814
Final Summary:   loss: 0.819
[19,  2000] loss: 0.812
[19,  4000] loss: 0.822
[19,  6000] loss: 0.824
[19,  8000] loss: 0.815
[19, 10000] loss: 0.802
[19, 12000] loss: 0.795
Final Summary:   loss: 0.811
[20,  2000] loss: 0.813
[20,  4000] loss: 0.794
[20,  6000] loss: 0.810
[20,  8000] loss: 0.805
[20, 10000] loss: 0.830
[20, 12000] loss: 0.800
Final Summary:   loss: 0.809
[21,  2000] loss: 0.800
[21,  4000] loss: 0.805
[21,  6000] loss: 0.789
[21,  8000] loss: 0.792
[21, 10000] loss: 0.802
[21, 12000] loss: 0.797
Final Summary:   loss: 0.798
Train Accuracy of the network: 72 %
Test Accuracy of the network: 72 %
[22,  2000] loss: 0.777
[22,  4000] loss: 0.759
[22,  6000] loss: 0.801
[22,  8000] loss: 0.797
[22, 10000] loss: 0.795
[22, 12000] loss: 0.796
Final Summary:   loss: 0.787
[23,  2000] loss: 0.781
[23,  4000] loss: 0.778
[23,  6000] loss: 0.777
[23,  8000] loss: 0.798
[23, 10000] loss: 0.779
[23, 12000] loss: 0.804
Final Summary:   loss: 0.785
[24,  2000] loss: 0.765
[24,  4000] loss: 0.787
[24,  6000] loss: 0.775
[24,  8000] loss: 0.766
[24, 10000] loss: 0.775
[24, 12000] loss: 0.777
Final Summary:   loss: 0.776
[25,  2000] loss: 0.748
[25,  4000] loss: 0.767
[25,  6000] loss: 0.784
[25,  8000] loss: 0.758
[25, 10000] loss: 0.779
[25, 12000] loss: 0.793
Final Summary:   loss: 0.772
[26,  2000] loss: 0.760
[26,  4000] loss: 0.784
[26,  6000] loss: 0.742
[26,  8000] loss: 0.777
[26, 10000] loss: 0.781
[26, 12000] loss: 0.760
Final Summary:   loss: 0.768
[27,  2000] loss: 0.759
[27,  4000] loss: 0.768
[27,  6000] loss: 0.754
[27,  8000] loss: 0.770
[27, 10000] loss: 0.768
[27, 12000] loss: 0.761
Final Summary:   loss: 0.763
[28,  2000] loss: 0.748
[28,  4000] loss: 0.754
[28,  6000] loss: 0.776
[28,  8000] loss: 0.766
[28, 10000] loss: 0.762
[28, 12000] loss: 0.763
Final Summary:   loss: 0.761
[29,  2000] loss: 0.732
[29,  4000] loss: 0.749
[29,  6000] loss: 0.760
[29,  8000] loss: 0.746
[29, 10000] loss: 0.775
[29, 12000] loss: 0.764
Final Summary:   loss: 0.754
[30,  2000] loss: 0.758
[30,  4000] loss: 0.762
[30,  6000] loss: 0.740
[30,  8000] loss: 0.765
[30, 10000] loss: 0.744
[30, 12000] loss: 0.756
Final Summary:   loss: 0.754
[31,  2000] loss: 0.752
[31,  4000] loss: 0.763
[31,  6000] loss: 0.740
[31,  8000] loss: 0.770
[31, 10000] loss: 0.740
[31, 12000] loss: 0.739
Final Summary:   loss: 0.750
[32,  2000] loss: 0.744
[32,  4000] loss: 0.755
[32,  6000] loss: 0.741
[32,  8000] loss: 0.741
[32, 10000] loss: 0.756
[32, 12000] loss: 0.758
Final Summary:   loss: 0.749
[33,  2000] loss: 0.729
[33,  4000] loss: 0.734
[33,  6000] loss: 0.757
[33,  8000] loss: 0.768
[33, 10000] loss: 0.750
[33, 12000] loss: 0.731
Final Summary:   loss: 0.744
[34,  2000] loss: 0.740
[34,  4000] loss: 0.739
[34,  6000] loss: 0.734
[34,  8000] loss: 0.736
[34, 10000] loss: 0.723
[34, 12000] loss: 0.749
Final Summary:   loss: 0.737
[35,  2000] loss: 0.747
[35,  4000] loss: 0.744
[35,  6000] loss: 0.727
[35,  8000] loss: 0.740
[35, 10000] loss: 0.762
[35, 12000] loss: 0.745
Final Summary:   loss: 0.745
[36,  2000] loss: 0.719
[36,  4000] loss: 0.737
[36,  6000] loss: 0.736
[36,  8000] loss: 0.739
[36, 10000] loss: 0.731
[36, 12000] loss: 0.740
Final Summary:   loss: 0.733
[37,  2000] loss: 0.702
[37,  4000] loss: 0.731
[37,  6000] loss: 0.730
[37,  8000] loss: 0.745
[37, 10000] loss: 0.722
[37, 12000] loss: 0.737
Final Summary:   loss: 0.728
[38,  2000] loss: 0.718
[38,  4000] loss: 0.737
[38,  6000] loss: 0.721
[38,  8000] loss: 0.724
[38, 10000] loss: 0.744
[38, 12000] loss: 0.760
Final Summary:   loss: 0.734
[39,  2000] loss: 0.711
[39,  4000] loss: 0.722
[39,  6000] loss: 0.722
[39,  8000] loss: 0.729
[39, 10000] loss: 0.734
[39, 12000] loss: 0.743
Final Summary:   loss: 0.728
[40,  2000] loss: 0.714
[40,  4000] loss: 0.723
[40,  6000] loss: 0.735
[40,  8000] loss: 0.744
[40, 10000] loss: 0.726
[40, 12000] loss: 0.705
Final Summary:   loss: 0.724
[41,  2000] loss: 0.711
[41,  4000] loss: 0.712
[41,  6000] loss: 0.718
[41,  8000] loss: 0.732
[41, 10000] loss: 0.740
[41, 12000] loss: 0.715
Final Summary:   loss: 0.721
Train Accuracy of the network: 74 %
Test Accuracy of the network: 72 %
[42,  2000] loss: 0.709
[42,  4000] loss: 0.690
[42,  6000] loss: 0.733
[42,  8000] loss: 0.735
[42, 10000] loss: 0.731
[42, 12000] loss: 0.711

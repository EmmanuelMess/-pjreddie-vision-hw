[1,  2000] loss: 2.302
[1,  4000] loss: 2.302
[1,  6000] loss: 2.283
[1,  8000] loss: 2.220
[1, 10000] loss: 2.202
[1, 12000] loss: 2.188
Final Summary:   loss: 2.247
Train Accuracy of the network: 26 %
Test Accuracy of the network: 28 %
[2,  2000] loss: 2.178
[2,  4000] loss: 2.161
[2,  6000] loss: 2.154
[2,  8000] loss: 2.151
[2, 10000] loss: 2.141
[2, 12000] loss: 2.140
Final Summary:   loss: 2.153
[3,  2000] loss: 2.126
[3,  4000] loss: 2.104
[3,  6000] loss: 2.095
[3,  8000] loss: 2.099
[3, 10000] loss: 2.081
[3, 12000] loss: 2.077
Final Summary:   loss: 2.097
[4,  2000] loss: 2.074
[4,  4000] loss: 2.068
[4,  6000] loss: 2.071
[4,  8000] loss: 2.060
[4, 10000] loss: 2.057
[4, 12000] loss: 2.061
Final Summary:   loss: 2.065
[5,  2000] loss: 2.052
[5,  4000] loss: 2.052
[5,  6000] loss: 2.044
[5,  8000] loss: 2.029
[5, 10000] loss: 2.041
[5, 12000] loss: 2.037
Final Summary:   loss: 2.043
[6,  2000] loss: 2.027
[6,  4000] loss: 2.032
[6,  6000] loss: 2.029
[6,  8000] loss: 2.031
[6, 10000] loss: 2.016
[6, 12000] loss: 2.023
Final Summary:   loss: 2.027
[7,  2000] loss: 2.019
[7,  4000] loss: 2.015
[7,  6000] loss: 2.010
[7,  8000] loss: 2.014
[7, 10000] loss: 2.009
[7, 12000] loss: 2.001
Final Summary:   loss: 2.011
[8,  2000] loss: 2.005
[8,  4000] loss: 1.995
[8,  6000] loss: 1.995
[8,  8000] loss: 1.990
[8, 10000] loss: 1.991
[8, 12000] loss: 1.997
Final Summary:   loss: 1.995
[9,  2000] loss: 1.996
[9,  4000] loss: 1.990
[9,  6000] loss: 1.981
[9,  8000] loss: 1.989
[9, 10000] loss: 1.977
[9, 12000] loss: 1.974
Final Summary:   loss: 1.985
[10,  2000] loss: 1.972
[10,  4000] loss: 1.980
[10,  6000] loss: 1.975
[10,  8000] loss: 1.964
[10, 10000] loss: 1.975
[10, 12000] loss: 1.973
Final Summary:   loss: 1.973
[11,  2000] loss: 1.971
[11,  4000] loss: 1.960
[11,  6000] loss: 1.959
[11,  8000] loss: 1.967
[11, 10000] loss: 1.965
[11, 12000] loss: 1.962
Final Summary:   loss: 1.963
[12,  2000] loss: 1.957
[12,  4000] loss: 1.951
[12,  6000] loss: 1.953
[12,  8000] loss: 1.947
[12, 10000] loss: 1.948
[12, 12000] loss: 1.958
Final Summary:   loss: 1.953
[13,  2000] loss: 1.955
[13,  4000] loss: 1.938
[13,  6000] loss: 1.945
[13,  8000] loss: 1.942
[13, 10000] loss: 1.946
[13, 12000] loss: 1.947
Final Summary:   loss: 1.947
[14,  2000] loss: 1.939
[14,  4000] loss: 1.941
[14,  6000] loss: 1.948
[14,  8000] loss: 1.936
[14, 10000] loss: 1.939
[14, 12000] loss: 1.933
Final Summary:   loss: 1.939
[15,  2000] loss: 1.941
[15,  4000] loss: 1.930
[15,  6000] loss: 1.940
[15,  8000] loss: 1.920
[15, 10000] loss: 1.934
[15, 12000] loss: 1.923
Final Summary:   loss: 1.932
[16,  2000] loss: 1.924
[16,  4000] loss: 1.929
[16,  6000] loss: 1.935
[16,  8000] loss: 1.922
[16, 10000] loss: 1.935
[16, 12000] loss: 1.936
Final Summary:   loss: 1.930
[17,  2000] loss: 1.928
[17,  4000] loss: 1.929
[17,  6000] loss: 1.917
[17,  8000] loss: 1.935
[17, 10000] loss: 1.934
[17, 12000] loss: 1.923
Final Summary:   loss: 1.928
[18,  2000] loss: 1.931
[18,  4000] loss: 1.928
[18,  6000] loss: 1.910
[18,  8000] loss: 1.917
[18, 10000] loss: 1.924
[18, 12000] loss: 1.914
Final Summary:   loss: 1.921
[19,  2000] loss: 1.919
[19,  4000] loss: 1.903
[19,  6000] loss: 1.914
[19,  8000] loss: 1.914
[19, 10000] loss: 1.918
[19, 12000] loss: 1.906
Final Summary:   loss: 1.912
[20,  2000] loss: 1.914
[20,  4000] loss: 1.901
[20,  6000] loss: 1.912
[20,  8000] loss: 1.910
[20, 10000] loss: 1.911
[20, 12000] loss: 1.908
Final Summary:   loss: 1.909
[21,  2000] loss: 1.911
[21,  4000] loss: 1.907
[21,  6000] loss: 1.903
[21,  8000] loss: 1.910
[21, 10000] loss: 1.910
[21, 12000] loss: 1.909
Final Summary:   loss: 1.908
Train Accuracy of the network: 53 %
Test Accuracy of the network: 54 %
[22,  2000] loss: 1.907
[22,  4000] loss: 1.900
[22,  6000] loss: 1.907
[22,  8000] loss: 1.903
[22, 10000] loss: 1.909
[22, 12000] loss: 1.902
Final Summary:   loss: 1.904
[23,  2000] loss: 1.902
[23,  4000] loss: 1.895
[23,  6000] loss: 1.908
[23,  8000] loss: 1.890
[23, 10000] loss: 1.908
[23, 12000] loss: 1.898
Final Summary:   loss: 1.900
[24,  2000] loss: 1.893
[24,  4000] loss: 1.895
[24,  6000] loss: 1.888
[24,  8000] loss: 1.905
[24, 10000] loss: 1.902
[24, 12000] loss: 1.901
Final Summary:   loss: 1.898
[25,  2000] loss: 1.883
[25,  4000] loss: 1.896
[25,  6000] loss: 1.891
[25,  8000] loss: 1.900
[25, 10000] loss: 1.895
[25, 12000] loss: 1.888
Final Summary:   loss: 1.892
[26,  2000] loss: 1.888
[26,  4000] loss: 1.900
[26,  6000] loss: 1.895
[26,  8000] loss: 1.888
[26, 10000] loss: 1.897
[26, 12000] loss: 1.892
Final Summary:   loss: 1.893
[27,  2000] loss: 1.891
[27,  4000] loss: 1.894
[27,  6000] loss: 1.884
[27,  8000] loss: 1.895
[27, 10000] loss: 1.888
[27, 12000] loss: 1.878
Final Summary:   loss: 1.888
[28,  2000] loss: 1.878
[28,  4000] loss: 1.891
[28,  6000] loss: 1.882
[28,  8000] loss: 1.884
[28, 10000] loss: 1.874
[28, 12000] loss: 1.888
Final Summary:   loss: 1.883
[29,  2000] loss: 1.875
[29,  4000] loss: 1.887
[29,  6000] loss: 1.888
[29,  8000] loss: 1.879
[29, 10000] loss: 1.873
[29, 12000] loss: 1.878
Final Summary:   loss: 1.880
[30,  2000] loss: 1.876
[30,  4000] loss: 1.871
[30,  6000] loss: 1.886
[30,  8000] loss: 1.877
[30, 10000] loss: 1.873
[30, 12000] loss: 1.873
Final Summary:   loss: 1.876
[31,  2000] loss: 1.869
[31,  4000] loss: 1.865
[31,  6000] loss: 1.885
[31,  8000] loss: 1.878
[31, 10000] loss: 1.872
[31, 12000] loss: 1.877
Final Summary:   loss: 1.874
[32,  2000] loss: 1.862
[32,  4000] loss: 1.872
[32,  6000] loss: 1.873
[32,  8000] loss: 1.878
[32, 10000] loss: 1.872
[32, 12000] loss: 1.873
Final Summary:   loss: 1.871
[33,  2000] loss: 1.864
[33,  4000] loss: 1.879
[33,  6000] loss: 1.870
[33,  8000] loss: 1.861
[33, 10000] loss: 1.869
[33, 12000] loss: 1.877
Final Summary:   loss: 1.871
[34,  2000] loss: 1.866
[34,  4000] loss: 1.873
[34,  6000] loss: 1.863
[34,  8000] loss: 1.867
[34, 10000] loss: 1.876
[34, 12000] loss: 1.873
Final Summary:   loss: 1.870
[35,  2000] loss: 1.862
[35,  4000] loss: 1.871
[35,  6000] loss: 1.862
[35,  8000] loss: 1.870
[35, 10000] loss: 1.875
[35, 12000] loss: 1.867
Final Summary:   loss: 1.868
[36,  2000] loss: 1.866
[36,  4000] loss: 1.861
[36,  6000] loss: 1.857
[36,  8000] loss: 1.867
[36, 10000] loss: 1.871
[36, 12000] loss: 1.861
Final Summary:   loss: 1.863
[37,  2000] loss: 1.861
[37,  4000] loss: 1.857
[37,  6000] loss: 1.859
[37,  8000] loss: 1.862
[37, 10000] loss: 1.862
[37, 12000] loss: 1.865
Final Summary:   loss: 1.862
[38,  2000] loss: 1.867
[38,  4000] loss: 1.856
[38,  6000] loss: 1.862
[38,  8000] loss: 1.855
[38, 10000] loss: 1.852
[38, 12000] loss: 1.854
Final Summary:   loss: 1.858
[39,  2000] loss: 1.858
[39,  4000] loss: 1.865
[39,  6000] loss: 1.870
[39,  8000] loss: 1.868
[39, 10000] loss: 1.857
[39, 12000] loss: 1.866
Final Summary:   loss: 1.864
[40,  2000] loss: 1.857
[40,  4000] loss: 1.859
[40,  6000] loss: 1.859
[40,  8000] loss: 1.852
[40, 10000] loss: 1.857
[40, 12000] loss: 1.866
Final Summary:   loss: 1.859
[41,  2000] loss: 1.854
[41,  4000] loss: 1.859
[41,  6000] loss: 1.865
[41,  8000] loss: 1.866
[41, 10000] loss: 1.853
[41, 12000] loss: 1.852
Final Summary:   loss: 1.858
Train Accuracy of the network: 61 %
Test Accuracy of the network: 60 %
[42,  2000] loss: 1.853
[42,  4000] loss: 1.864
[42,  6000] loss: 1.857
[42,  8000] loss: 1.864
[42, 10000] loss: 1.857
[42, 12000] loss: 1.859
Final Summary:   loss: 1.859
[43,  2000] loss: 1.856
[43,  4000] loss: 1.851
[43,  6000] loss: 1.869
[43,  8000] loss: 1.855
[43, 10000] loss: 1.860
[43, 12000] loss: 1.856
Final Summary:   loss: 1.858
[44,  2000] loss: 1.859
[44,  4000] loss: 1.847
[44,  6000] loss: 1.858
[44,  8000] loss: 1.855
[44, 10000] loss: 1.858
[44, 12000] loss: 1.868
Final Summary:   loss: 1.858
[45,  2000] loss: 1.852
[45,  4000] loss: 1.862
[45,  6000] loss: 1.845
[45,  8000] loss: 1.847
[45, 10000] loss: 1.846
[45, 12000] loss: 1.863
Final Summary:   loss: 1.852
[46,  2000] loss: 1.850
[46,  4000] loss: 1.851
[46,  6000] loss: 1.857
[46,  8000] loss: 1.850
[46, 10000] loss: 1.852
[46, 12000] loss: 1.858
Final Summary:   loss: 1.853
[47,  2000] loss: 1.853
[47,  4000] loss: 1.854
[47,  6000] loss: 1.856
[47,  8000] loss: 1.865
[47, 10000] loss: 1.859
[47, 12000] loss: 1.852
Final Summary:   loss: 1.857
[48,  2000] loss: 1.861
[48,  4000] loss: 1.863
[48,  6000] loss: 1.855
[48,  8000] loss: 1.870
[48, 10000] loss: 1.846
[48, 12000] loss: 1.856
Final Summary:   loss: 1.859
[49,  2000] loss: 1.851
[49,  4000] loss: 1.862
[49,  6000] loss: 1.865
[49,  8000] loss: 1.847
[49, 10000] loss: 1.860
[49, 12000] loss: 1.864
Final Summary:   loss: 1.859
[50,  2000] loss: 1.854
[50,  4000] loss: 1.864
[50,  6000] loss: 1.843
[50,  8000] loss: 1.847
[50, 10000] loss: 1.855
[50, 12000] loss: 1.858
Final Summary:   loss: 1.854
[51,  2000] loss: 1.854
[51,  4000] loss: 1.842
[51,  6000] loss: 1.842
[51,  8000] loss: 1.850
[51, 10000] loss: 1.865
[51, 12000] loss: 1.848
Final Summary:   loss: 1.850
[52,  2000] loss: 1.847
[52,  4000] loss: 1.840
[52,  6000] loss: 1.855
[52,  8000] loss: 1.836
[52, 10000] loss: 1.862
[52, 12000] loss: 1.850
Final Summary:   loss: 1.849
[53,  2000] loss: 1.844
[53,  4000] loss: 1.848
[53,  6000] loss: 1.840
[53,  8000] loss: 1.844
[53, 10000] loss: 1.843
[53, 12000] loss: 1.848
Final Summary:   loss: 1.844
[54,  2000] loss: 1.848
[54,  4000] loss: 1.849
[54,  6000] loss: 1.844
[54,  8000] loss: 1.847
[54, 10000] loss: 1.837
[54, 12000] loss: 1.850
Final Summary:   loss: 1.845
[55,  2000] loss: 1.855
[55,  4000] loss: 1.848
[55,  6000] loss: 1.850
[55,  8000] loss: 1.849
[55, 10000] loss: 1.858
[55, 12000] loss: 1.843
Final Summary:   loss: 1.851
[56,  2000] loss: 1.859
[56,  4000] loss: 1.851
[56,  6000] loss: 1.848
[56,  8000] loss: 1.842
[56, 10000] loss: 1.850
[56, 12000] loss: 1.849
Final Summary:   loss: 1.850
[57,  2000] loss: 1.851
[57,  4000] loss: 1.853
[57,  6000] loss: 1.847
[57,  8000] loss: 1.852
[57, 10000] loss: 1.845
[57, 12000] loss: 1.857
Final Summary:   loss: 1.851
[58,  2000] loss: 1.873
[58,  4000] loss: 1.854
[58,  6000] loss: 1.841
[58,  8000] loss: 1.845
[58, 10000] loss: 1.855
[58, 12000] loss: 1.847
Final Summary:   loss: 1.852
[59,  2000] loss: 1.849
[59,  4000] loss: 1.855
[59,  6000] loss: 1.829
[59,  8000] loss: 1.853
[59, 10000] loss: 1.851
[59, 12000] loss: 1.855
Final Summary:   loss: 1.849
[60,  2000] loss: 1.841
[60,  4000] loss: 1.854
[60,  6000] loss: 1.837
[60,  8000] loss: 1.845
[60, 10000] loss: 1.849
[60, 12000] loss: 1.851
Final Summary:   loss: 1.848
[61,  2000] loss: 1.852
[61,  4000] loss: 1.841
[61,  6000] loss: 1.846
[61,  8000] loss: 1.850
[61, 10000] loss: 1.850
[61, 12000] loss: 1.859
Final Summary:   loss: 1.851
Train Accuracy of the network: 61 %
Test Accuracy of the network: 61 %
[62,  2000] loss: 1.847
[62,  4000] loss: 1.848
[62,  6000] loss: 1.859
[62,  8000] loss: 1.845
[62, 10000] loss: 1.855
[62, 12000] loss: 1.862
Final Summary:   loss: 1.852
[63,  2000] loss: 1.851
[63,  4000] loss: 1.849
[63,  6000] loss: 1.857
[63,  8000] loss: 1.840
[63, 10000] loss: 1.832
[63, 12000] loss: 1.843
Final Summary:   loss: 1.845
[64,  2000] loss: 1.849
[64,  4000] loss: 1.853
[64,  6000] loss: 1.866
[64,  8000] loss: 1.852
[64, 10000] loss: 1.845
[64, 12000] loss: 1.869
Final Summary:   loss: 1.855
[65,  2000] loss: 1.841
[65,  4000] loss: 1.850
[65,  6000] loss: 1.850
[65,  8000] loss: 1.844
[65, 10000] loss: 1.853
[65, 12000] loss: 1.839
Final Summary:   loss: 1.846
[66,  2000] loss: 1.850
[66,  4000] loss: 1.845
[66,  6000] loss: 1.853
[66,  8000] loss: 1.854
[66, 10000] loss: 1.846
[66, 12000] loss: 1.843
Final Summary:   loss: 1.849
[67,  2000] loss: 1.853
[67,  4000] loss: 1.859
[67,  6000] loss: 1.849
[67,  8000] loss: 1.841
[67, 10000] loss: 1.847
[67, 12000] loss: 1.859
Final Summary:   loss: 1.851
[68,  2000] loss: 1.840
[68,  4000] loss: 1.866
[68,  6000] loss: 1.844
[68,  8000] loss: 1.842
[68, 10000] loss: 1.839
[68, 12000] loss: 1.848
Final Summary:   loss: 1.847
[69,  2000] loss: 1.863
[69,  4000] loss: 1.829
[69,  6000] loss: 1.857
[69,  8000] loss: 1.868
[69, 10000] loss: 1.848
[69, 12000] loss: 1.855
Final Summary:   loss: 1.853
[70,  2000] loss: 1.844
[70,  4000] loss: 1.841
[70,  6000] loss: 1.845
[70,  8000] loss: 1.850
[70, 10000] loss: 1.852
[70, 12000] loss: 1.845
Final Summary:   loss: 1.847
[71,  2000] loss: 1.852
[71,  4000] loss: 1.857
[71,  6000] loss: 1.841
[71,  8000] loss: 1.843
[71, 10000] loss: 1.847
[71, 12000] loss: 1.845
Final Summary:   loss: 1.848
[72,  2000] loss: 1.864
[72,  4000] loss: 1.848
[72,  6000] loss: 1.826
[72,  8000] loss: 1.834
[72, 10000] loss: 1.843
[72, 12000] loss: 1.835
Final Summary:   loss: 1.841
[73,  2000] loss: 1.839
[73,  4000] loss: 1.831
[73,  6000] loss: 1.854
[73,  8000] loss: 1.853
[73, 10000] loss: 1.843
[73, 12000] loss: 1.846
Final Summary:   loss: 1.844
[74,  2000] loss: 1.850
[74,  4000] loss: 1.849
[74,  6000] loss: 1.847
[74,  8000] loss: 1.848
[74, 10000] loss: 1.838
[74, 12000] loss: 1.845
Final Summary:   loss: 1.847
[75,  2000] loss: 1.846
[75,  4000] loss: 1.841
[75,  6000] loss: 1.833
[75,  8000] loss: 1.839
[75, 10000] loss: 1.858
[75, 12000] loss: 1.835
Final Summary:   loss: 1.843
[76,  2000] loss: 1.857
[76,  4000] loss: 1.843
[76,  6000] loss: 1.848
[76,  8000] loss: 1.851
[76, 10000] loss: 1.856
[76, 12000] loss: 1.846
Final Summary:   loss: 1.851
[77,  2000] loss: 1.835
[77,  4000] loss: 1.844
[77,  6000] loss: 1.850
[77,  8000] loss: 1.853
[77, 10000] loss: 1.857
[77, 12000] loss: 1.844
Final Summary:   loss: 1.847
[78,  2000] loss: 1.839
[78,  4000] loss: 1.846
[78,  6000] loss: 1.838
[78,  8000] loss: 1.870
[78, 10000] loss: 1.867
[78, 12000] loss: 1.851
Final Summary:   loss: 1.851
[79,  2000] loss: 1.858
[79,  4000] loss: 1.871
[79,  6000] loss: 1.864
[79,  8000] loss: 1.852
[79, 10000] loss: 1.846
[79, 12000] loss: 1.853
Final Summary:   loss: 1.857
[80,  2000] loss: 1.849
[80,  4000] loss: 1.857
[80,  6000] loss: 1.883
[80,  8000] loss: 1.868
[80, 10000] loss: 1.866
[80, 12000] loss: 1.849
Final Summary:   loss: 1.862
[81,  2000] loss: 1.849
[81,  4000] loss: 1.848
[81,  6000] loss: 1.845
[81,  8000] loss: 1.856
[81, 10000] loss: 1.875
[81, 12000] loss: 1.856
Final Summary:   loss: 1.855
Train Accuracy of the network: 61 %
Test Accuracy of the network: 62 %
[82,  2000] loss: 1.853
[82,  4000] loss: 1.852
[82,  6000] loss: 1.851
[82,  8000] loss: 1.866
[82, 10000] loss: 1.845
[82, 12000] loss: 1.838
Final Summary:   loss: 1.852
[83,  2000] loss: 1.852
[83,  4000] loss: 1.843
[83,  6000] loss: 1.862
[83,  8000] loss: 1.844
[83, 10000] loss: 1.859
[83, 12000] loss: 1.865
Final Summary:   loss: 1.855
[84,  2000] loss: 1.869
[84,  4000] loss: 1.868
[84,  6000] loss: 1.856
[84,  8000] loss: 1.857
[84, 10000] loss: 1.849
[84, 12000] loss: 1.852
Final Summary:   loss: 1.858
[85,  2000] loss: 1.851
[85,  4000] loss: 1.853
[85,  6000] loss: 1.862
[85,  8000] loss: 1.868
[85, 10000] loss: 1.861
[85, 12000] loss: 1.844
Final Summary:   loss: 1.857
[86,  2000] loss: 1.838
[86,  4000] loss: 1.845
[86,  6000] loss: 1.868
[86,  8000] loss: 1.870
[86, 10000] loss: 1.861
[86, 12000] loss: 1.875
Final Summary:   loss: 1.860
[87,  2000] loss: 1.870
[87,  4000] loss: 1.855
[87,  6000] loss: 1.844
[87,  8000] loss: 1.856
[87, 10000] loss: 1.852
[87, 12000] loss: 1.854
Final Summary:   loss: 1.856
[88,  2000] loss: 1.857
[88,  4000] loss: 1.864
[88,  6000] loss: 1.876
[88,  8000] loss: 1.880
[88, 10000] loss: 1.870
[88, 12000] loss: 1.870
Final Summary:   loss: 1.869
[89,  2000] loss: 1.862
[89,  4000] loss: 1.850
[89,  6000] loss: 1.863
[89,  8000] loss: 1.848
[89, 10000] loss: 1.870
[89, 12000] loss: 1.882
Final Summary:   loss: 1.863
[90,  2000] loss: 1.858
[90,  4000] loss: 1.860
[90,  6000] loss: 1.866
[90,  8000] loss: 1.861
[90, 10000] loss: 1.856
[90, 12000] loss: 1.898
Final Summary:   loss: 1.867
[91,  2000] loss: 1.869
[91,  4000] loss: 1.858
[91,  6000] loss: 1.847
[91,  8000] loss: 1.853
[91, 10000] loss: 1.869
[91, 12000] loss: 1.862
Final Summary:   loss: 1.860
[92,  2000] loss: 1.868
[92,  4000] loss: 1.856
[92,  6000] loss: 1.847
[92,  8000] loss: 1.865
[92, 10000] loss: 1.855
[92, 12000] loss: 1.878
Final Summary:   loss: 1.861
[93,  2000] loss: 1.849
[93,  4000] loss: 1.852
[93,  6000] loss: 1.849
[93,  8000] loss: 1.872
[93, 10000] loss: 1.863
[93, 12000] loss: 1.872
Final Summary:   loss: 1.861
[94,  2000] loss: 1.841
[94,  4000] loss: 1.847
[94,  6000] loss: 1.858
[94,  8000] loss: 1.846
[94, 10000] loss: 1.856
[94, 12000] loss: 1.861
Final Summary:   loss: 1.852
[95,  2000] loss: 1.856
[95,  4000] loss: 1.867
[95,  6000] loss: 1.879
[95,  8000] loss: 1.874
[95, 10000] loss: 1.853
[95, 12000] loss: 1.856
Final Summary:   loss: 1.864
[96,  2000] loss: 1.864
[96,  4000] loss: 1.858
[96,  6000] loss: 1.881
[96,  8000] loss: 1.874
[96, 10000] loss: 1.870
[96, 12000] loss: 1.893
Final Summary:   loss: 1.873
[97,  2000] loss: 1.857
[97,  4000] loss: 1.855
[97,  6000] loss: 1.891
[97,  8000] loss: 1.872
[97, 10000] loss: 1.881
[97, 12000] loss: 1.879
Final Summary:   loss: 1.871
[98,  2000] loss: 1.863
[98,  4000] loss: 1.862
[98,  6000] loss: 1.885
[98,  8000] loss: 1.876
[98, 10000] loss: 1.883
[98, 12000] loss: 1.886
Final Summary:   loss: 1.877
[99,  2000] loss: 1.866
[99,  4000] loss: 1.851
[99,  6000] loss: 1.890
[99,  8000] loss: 1.882
[99, 10000] loss: 1.876
[99, 12000] loss: 1.900
Final Summary:   loss: 1.881
[100,  2000] loss: 1.886
[100,  4000] loss: 1.908
[100,  6000] loss: 1.872
[100,  8000] loss: 1.865
[100, 10000] loss: 1.871
[100, 12000] loss: 1.858
Final Summary:   loss: 1.877
[101,  2000] loss: 1.870
[101,  4000] loss: 1.890
[101,  6000] loss: 1.846
[101,  8000] loss: 1.870
[101, 10000] loss: 1.877
[101, 12000] loss: 1.862
Final Summary:   loss: 1.870
Train Accuracy of the network: 59 %
Test Accuracy of the network: 58 %
[102,  2000] loss: 1.885
[102,  4000] loss: 1.869
[102,  6000] loss: 1.889
[102,  8000] loss: 1.868
[102, 10000] loss: 1.868
[102, 12000] loss: 1.878
Final Summary:   loss: 1.876
[103,  2000] loss: 1.857
[103,  4000] loss: 1.872
[103,  6000] loss: 1.864
[103,  8000] loss: 1.876
[103, 10000] loss: 1.872
[103, 12000] loss: 1.887
Final Summary:   loss: 1.871
[104,  2000] loss: 1.881
[104,  4000] loss: 1.882
[104,  6000] loss: 1.870
[104,  8000] loss: 1.877
[104, 10000] loss: 1.875
[104, 12000] loss: 1.875
Final Summary:   loss: 1.877
[105,  2000] loss: 1.889
[105,  4000] loss: 1.899
[105,  6000] loss: 1.892
[105,  8000] loss: 1.881
[105, 10000] loss: 1.880
[105, 12000] loss: 1.876
Final Summary:   loss: 1.886
[106,  2000] loss: 1.864
[106,  4000] loss: 1.879
[106,  6000] loss: 1.871
[106,  8000] loss: 1.864
[106, 10000] loss: 1.871
[106, 12000] loss: 1.869
Final Summary:   loss: 1.870
[107,  2000] loss: 1.871
[107,  4000] loss: 1.881
[107,  6000] loss: 1.864
[107,  8000] loss: 1.885
[107, 10000] loss: 1.870
[107, 12000] loss: 1.862
Final Summary:   loss: 1.873
[108,  2000] loss: 1.866
[108,  4000] loss: 1.874
[108,  6000] loss: 1.889
[108,  8000] loss: 1.855
[108, 10000] loss: 1.863
[108, 12000] loss: 1.860
Final Summary:   loss: 1.868
[109,  2000] loss: 1.859
[109,  4000] loss: 1.879
[109,  6000] loss: 1.901
[109,  8000] loss: 1.916
[109, 10000] loss: 1.883
[109, 12000] loss: 1.862
Final Summary:   loss: 1.882
[110,  2000] loss: 1.881
[110,  4000] loss: 1.857
[110,  6000] loss: 1.884
[110,  8000] loss: 1.865
[110, 10000] loss: 1.875
[110, 12000] loss: 1.862
Final Summary:   loss: 1.870
[111,  2000] loss: 1.861
[111,  4000] loss: 1.865
[111,  6000] loss: 1.899
[111,  8000] loss: 1.864
[111, 10000] loss: 1.858
[111, 12000] loss: 1.867
Final Summary:   loss: 1.869
[112,  2000] loss: 1.862
[112,  4000] loss: 1.871
[112,  6000] loss: 1.878
[112,  8000] loss: 1.872
[112, 10000] loss: 1.873
[112, 12000] loss: 1.886
Final Summary:   loss: 1.874
[113,  2000] loss: 1.872
[113,  4000] loss: 1.890
[113,  6000] loss: 1.940
[113,  8000] loss: 1.887
[113, 10000] loss: 1.871
[113, 12000] loss: 1.883
Final Summary:   loss: 1.890
[114,  2000] loss: 1.892
[114,  4000] loss: 1.901
[114,  6000] loss: 1.868
[114,  8000] loss: 1.867
[114, 10000] loss: 1.896
[114, 12000] loss: 1.891
Final Summary:   loss: 1.886
[115,  2000] loss: 1.868
[115,  4000] loss: 1.860
[115,  6000] loss: 1.891
[115,  8000] loss: 1.882
[115, 10000] loss: 1.887
[115, 12000] loss: 1.863
Final Summary:   loss: 1.875
[116,  2000] loss: 1.869
[116,  4000] loss: 1.877
[116,  6000] loss: 1.908
[116,  8000] loss: 1.886
[116, 10000] loss: 1.874
[116, 12000] loss: 1.885
Final Summary:   loss: 1.885
[117,  2000] loss: 1.896
[117,  4000] loss: 1.895
[117,  6000] loss: 1.889
[117,  8000] loss: 1.875
[117, 10000] loss: 1.883
[117, 12000] loss: 1.882
Final Summary:   loss: 1.886
[118,  2000] loss: 1.906
[118,  4000] loss: 1.892
[118,  6000] loss: 1.895
[118,  8000] loss: 1.877
[118, 10000] loss: 1.865
[118, 12000] loss: 1.876
Final Summary:   loss: 1.886
[119,  2000] loss: 1.866
[119,  4000] loss: 1.882
[119,  6000] loss: 1.878
[119,  8000] loss: 1.872
[119, 10000] loss: 1.873
[119, 12000] loss: 1.912
Final Summary:   loss: 1.883
[120,  2000] loss: 1.902
[120,  4000] loss: 1.951
[120,  6000] loss: 1.912
[120,  8000] loss: 1.923
[120, 10000] loss: 1.907
[120, 12000] loss: 1.915
Final Summary:   loss: 1.919
[121,  2000] loss: 1.915
[121,  4000] loss: 1.903
[121,  6000] loss: 1.917
[121,  8000] loss: 1.924
[121, 10000] loss: 1.915
[121, 12000] loss: 1.889
Final Summary:   loss: 1.909
Train Accuracy of the network: 56 %
Test Accuracy of the network: 58 %
[122,  2000] loss: 1.905
[122,  4000] loss: 1.936
[122,  6000] loss: 1.923
[122,  8000] loss: 1.907
[122, 10000] loss: 1.909
[122, 12000] loss: 1.892
Final Summary:   loss: 1.911
[123,  2000] loss: 1.900
[123,  4000] loss: 1.926
[123,  6000] loss: 1.916
[123,  8000] loss: 1.884
[123, 10000] loss: 1.897
[123, 12000] loss: 1.887
Final Summary:   loss: 1.901
[124,  2000] loss: 1.872
[124,  4000] loss: 1.932
[124,  6000] loss: 1.882
[124,  8000] loss: 1.909
[124, 10000] loss: 1.936
[124, 12000] loss: 1.919
Final Summary:   loss: 1.909
[125,  2000] loss: 1.928
[125,  4000] loss: 1.908
[125,  6000] loss: 1.888
[125,  8000] loss: 1.916
[125, 10000] loss: 1.899
[125, 12000] loss: 1.909
Final Summary:   loss: 1.911
[126,  2000] loss: 1.923
[126,  4000] loss: 1.920
[126,  6000] loss: 1.955
[126,  8000] loss: 1.923
[126, 10000] loss: 1.912
[126, 12000] loss: 1.925
Final Summary:   loss: 1.928
[127,  2000] loss: 1.910
[127,  4000] loss: 1.917
[127,  6000] loss: 1.911
[127,  8000] loss: 1.892
[127, 10000] loss: 1.896
[127, 12000] loss: 1.907
Final Summary:   loss: 1.906
[128,  2000] loss: 1.921
[128,  4000] loss: 1.962
[128,  6000] loss: 1.901
[128,  8000] loss: 1.909
[128, 10000] loss: 1.893
[128, 12000] loss: 1.889
Final Summary:   loss: 1.912
[129,  2000] loss: 1.891
[129,  4000] loss: 1.901
[129,  6000] loss: 1.896
[129,  8000] loss: 1.891
[129, 10000] loss: 1.903
[129, 12000] loss: 1.912
Final Summary:   loss: 1.901
[130,  2000] loss: 1.923
[130,  4000] loss: 1.908
[130,  6000] loss: 1.914
[130,  8000] loss: 1.924
[130, 10000] loss: 1.917
[130, 12000] loss: 1.928
Final Summary:   loss: 1.919
[131,  2000] loss: 1.918
[131,  4000] loss: 1.911
[131,  6000] loss: 1.900
[131,  8000] loss: 1.891
[131, 10000] loss: 1.898
[131, 12000] loss: 1.937
Final Summary:   loss: 1.910
[132,  2000] loss: 1.916
[132,  4000] loss: 1.934
[132,  6000] loss: 1.923
[132,  8000] loss: 1.904
[132, 10000] loss: 1.890
[132, 12000] loss: 1.900
Final Summary:   loss: 1.911
[133,  2000] loss: 1.922
[133,  4000] loss: 1.939
[133,  6000] loss: 1.895
[133,  8000] loss: 1.979
[133, 10000] loss: 1.923
[133, 12000] loss: 1.945
Final Summary:   loss: 1.936
[134,  2000] loss: 1.932
[134,  4000] loss: 1.919
[134,  6000] loss: 1.949
[134,  8000] loss: 1.964
[134, 10000] loss: 2.003
[134, 12000] loss: 2.000
Final Summary:   loss: 1.963
[135,  2000] loss: 1.970
[135,  4000] loss: 1.946
[135,  6000] loss: 1.935
[135,  8000] loss: 1.926
[135, 10000] loss: 1.953
[135, 12000] loss: 1.942
Final Summary:   loss: 1.944
[136,  2000] loss: 1.918
[136,  4000] loss: 1.964
[136,  6000] loss: 1.967
[136,  8000] loss: 1.934
[136, 10000] loss: 1.929
[136, 12000] loss: 1.956
Final Summary:   loss: 1.944
[137,  2000] loss: 1.900
[137,  4000] loss: 1.923
[137,  6000] loss: 1.952
[137,  8000] loss: 1.931
[137, 10000] loss: 1.906
[137, 12000] loss: 1.915
Final Summary:   loss: 1.923
[138,  2000] loss: 1.961
[138,  4000] loss: 1.969
[138,  6000] loss: 1.944
[138,  8000] loss: 1.935
[138, 10000] loss: 1.919
[138, 12000] loss: 1.930
Final Summary:   loss: 1.942
[139,  2000] loss: 1.924
[139,  4000] loss: 1.930
[139,  6000] loss: 1.927
[139,  8000] loss: 1.917
[139, 10000] loss: 1.921
[139, 12000] loss: 1.955
Final Summary:   loss: 1.932
[140,  2000] loss: 1.978
[140,  4000] loss: 1.934
[140,  6000] loss: 1.925
[140,  8000] loss: 1.931
[140, 10000] loss: 1.944
[140, 12000] loss: 1.967
Final Summary:   loss: 1.946
[141,  2000] loss: 1.927
[141,  4000] loss: 1.946
[141,  6000] loss: 1.945
[141,  8000] loss: 1.986
[141, 10000] loss: 2.039
[141, 12000] loss: 1.941
Final Summary:   loss: 1.963
Train Accuracy of the network: 54 %
Test Accuracy of the network: 55 %
[142,  2000] loss: 1.915
[142,  4000] loss: 1.922
[142,  6000] loss: 1.956
[142,  8000] loss: 1.951
[142, 10000] loss: 1.945
[142, 12000] loss: 1.908
Final Summary:   loss: 1.934
[143,  2000] loss: 1.933
[143,  4000] loss: 1.933
[143,  6000] loss: 2.099
[143,  8000] loss: 1.989
[143, 10000] loss: 1.920
[143, 12000] loss: 1.926
Final Summary:   loss: 1.965
[144,  2000] loss: 1.949
[144,  4000] loss: 1.916
[144,  6000] loss: 1.956
[144,  8000] loss: 1.960
[144, 10000] loss: 1.990
[144, 12000] loss: 1.933
Final Summary:   loss: 1.949
[145,  2000] loss: 1.928
[145,  4000] loss: 1.939
[145,  6000] loss: 1.949
[145,  8000] loss: 1.935
[145, 10000] loss: 1.941
[145, 12000] loss: 1.942
Final Summary:   loss: 1.941
[146,  2000] loss: 1.947
[146,  4000] loss: 1.998
[146,  6000] loss: 1.948
[146,  8000] loss: 1.962
[146, 10000] loss: 1.953
[146, 12000] loss: 1.949
Final Summary:   loss: 1.959
[147,  2000] loss: 1.943
[147,  4000] loss: 1.950
[147,  6000] loss: 1.953
[147,  8000] loss: 1.967
[147, 10000] loss: 1.978
[147, 12000] loss: 1.934
Final Summary:   loss: 1.954
[148,  2000] loss: 1.967
[148,  4000] loss: 1.938
[148,  6000] loss: 1.933
[148,  8000] loss: 1.958
[148, 10000] loss: 1.966
[148, 12000] loss: 1.978
Final Summary:   loss: 1.957
[149,  2000] loss: 1.977
[149,  4000] loss: 1.940
[149,  6000] loss: 1.938
[149,  8000] loss: 1.921
[149, 10000] loss: 1.951
[149, 12000] loss: 1.960
Final Summary:   loss: 1.948
[150,  2000] loss: 1.951
[150,  4000] loss: 1.970
[150,  6000] loss: 1.963
[150,  8000] loss: 1.986
[150, 10000] loss: 1.982
[150, 12000] loss: 1.975
Final Summary:   loss: 1.971
[151,  2000] loss: 1.954
[151,  4000] loss: 1.993
[151,  6000] loss: 1.946
[151,  8000] loss: 1.948
[151, 10000] loss: 1.970
[151, 12000] loss: 2.005
Final Summary:   loss: 1.971
[152,  2000] loss: 1.994
[152,  4000] loss: 1.944
[152,  6000] loss: 1.967
[152,  8000] loss: 1.963
[152, 10000] loss: 1.973
[152, 12000] loss: 1.958
Final Summary:   loss: 1.966
[153,  2000] loss: 1.936
[153,  4000] loss: 1.933
[153,  6000] loss: 1.915
[153,  8000] loss: 1.931
[153, 10000] loss: 1.942
[153, 12000] loss: 1.973
Final Summary:   loss: 1.939
[154,  2000] loss: 1.939
[154,  4000] loss: 1.945
[154,  6000] loss: 1.951
[154,  8000] loss: 1.944
[154, 10000] loss: 1.933
[154, 12000] loss: 1.944
Final Summary:   loss: 1.943
[155,  2000] loss: 1.951
[155,  4000] loss: 1.948
[155,  6000] loss: 1.927
[155,  8000] loss: 1.918
[155, 10000] loss: 1.929
[155, 12000] loss: 1.945
Final Summary:   loss: 1.937
[156,  2000] loss: 1.942
[156,  4000] loss: 1.928
[156,  6000] loss: 1.975
[156,  8000] loss: 2.011
[156, 10000] loss: 1.937
[156, 12000] loss: 1.926
Final Summary:   loss: 1.953
[157,  2000] loss: 1.947
[157,  4000] loss: 1.930
[157,  6000] loss: 1.956
[157,  8000] loss: 1.915
[157, 10000] loss: 1.934
[157, 12000] loss: 1.930
Final Summary:   loss: 1.934
[158,  2000] loss: 1.946
[158,  4000] loss: 1.925
[158,  6000] loss: 1.926
[158,  8000] loss: 1.973
[158, 10000] loss: 1.930
[158, 12000] loss: 1.913
Final Summary:   loss: 1.934
[159,  2000] loss: 1.925
[159,  4000] loss: 1.950
[159,  6000] loss: 1.958
[159,  8000] loss: 2.000
[159, 10000] loss: 1.930
[159, 12000] loss: 1.927
Final Summary:   loss: 1.950
[160,  2000] loss: 1.988
[160,  4000] loss: 1.931
[160,  6000] loss: 1.922
[160,  8000] loss: 1.926
[160, 10000] loss: 1.929
[160, 12000] loss: 1.917
Final Summary:   loss: 1.935
[161,  2000] loss: 1.906
[161,  4000] loss: 1.931
[161,  6000] loss: 1.926
[161,  8000] loss: 1.919
[161, 10000] loss: 1.932
[161, 12000] loss: 1.919
Final Summary:   loss: 1.923
Train Accuracy of the network: 54 %
Test Accuracy of the network: 55 %
[162,  2000] loss: 1.928
[162,  4000] loss: 1.905
[162,  6000] loss: 1.952
[162,  8000] loss: 1.934
[162, 10000] loss: 1.924
[162, 12000] loss: 1.946
Final Summary:   loss: 1.934
[163,  2000] loss: 1.930
[163,  4000] loss: 1.961
[163,  6000] loss: 1.928
[163,  8000] loss: 1.954
[163, 10000] loss: 1.958
[163, 12000] loss: 1.949
Final Summary:   loss: 1.947
[164,  2000] loss: 1.968
[164,  4000] loss: 1.945
[164,  6000] loss: 1.925
[164,  8000] loss: 1.963
[164, 10000] loss: 1.963
[164, 12000] loss: 1.947
Final Summary:   loss: 1.951
[165,  2000] loss: 1.927
[165,  4000] loss: 1.953
[165,  6000] loss: 1.982
[165,  8000] loss: 1.999
[165, 10000] loss: 1.937
[165, 12000] loss: 1.950
Final Summary:   loss: 1.958
[166,  2000] loss: 1.917
[166,  4000] loss: 1.951
[166,  6000] loss: 1.964
[166,  8000] loss: 1.981
[166, 10000] loss: 1.974
[166, 12000] loss: 1.941
Final Summary:   loss: 1.956
[167,  2000] loss: 1.959
[167,  4000] loss: 1.964
[167,  6000] loss: 1.954
[167,  8000] loss: 1.943
[167, 10000] loss: 1.975
[167, 12000] loss: 1.998
Final Summary:   loss: 1.967
[168,  2000] loss: 1.990
[168,  4000] loss: 1.951
[168,  6000] loss: 1.933
[168,  8000] loss: 1.947
[168, 10000] loss: 1.940
[168, 12000] loss: 1.937
Final Summary:   loss: 1.948
[169,  2000] loss: 1.936
[169,  4000] loss: 1.929
[169,  6000] loss: 1.932
[169,  8000] loss: 1.961
[169, 10000] loss: 1.944
[169, 12000] loss: 2.002
Final Summary:   loss: 1.957
[170,  2000] loss: 2.124
[170,  4000] loss: 2.098
[170,  6000] loss: 1.973
[170,  8000] loss: 1.965
[170, 10000] loss: 1.984
[170, 12000] loss: 1.960
Final Summary:   loss: 2.014
[171,  2000] loss: 1.952
[171,  4000] loss: 1.932
[171,  6000] loss: 1.920
[171,  8000] loss: 1.932
[171, 10000] loss: 1.939
[171, 12000] loss: 1.896
Final Summary:   loss: 1.929
[172,  2000] loss: 1.919
[172,  4000] loss: 1.946
[172,  6000] loss: 1.933
[172,  8000] loss: 1.918
[172, 10000] loss: 1.924
[172, 12000] loss: 1.926
Final Summary:   loss: 1.928
[173,  2000] loss: 1.952
[173,  4000] loss: 1.954
[173,  6000] loss: 1.958
[173,  8000] loss: 1.949
[173, 10000] loss: 1.944
[173, 12000] loss: 1.927
Final Summary:   loss: 1.952
[174,  2000] loss: 1.959
[174,  4000] loss: 1.961
[174,  6000] loss: 1.956
[174,  8000] loss: 1.940
[174, 10000] loss: 1.964
[174, 12000] loss: 1.958
Final Summary:   loss: 1.957
[175,  2000] loss: 1.965
[175,  4000] loss: 1.973
[175,  6000] loss: 1.988
[175,  8000] loss: 1.954
[175, 10000] loss: 1.956
[175, 12000] loss: 2.026
Final Summary:   loss: 1.979
[176,  2000] loss: 2.000
[176,  4000] loss: 2.021
[176,  6000] loss: 2.052
[176,  8000] loss: 2.080
[176, 10000] loss: 2.068
[176, 12000] loss: 2.049
Final Summary:   loss: 2.045
[177,  2000] loss: 2.022
[177,  4000] loss: 1.981
[177,  6000] loss: 1.971
[177,  8000] loss: 1.968
[177, 10000] loss: 1.955
[177, 12000] loss: 1.967
Final Summary:   loss: 1.977
[178,  2000] loss: 1.963
[178,  4000] loss: 1.934
[178,  6000] loss: 1.982
[178,  8000] loss: 1.968
[178, 10000] loss: 1.989
[178, 12000] loss: 1.952
Final Summary:   loss: 1.965
[179,  2000] loss: 1.973
[179,  4000] loss: 1.957
[179,  6000] loss: 1.944
[179,  8000] loss: 2.025
[179, 10000] loss: 2.000
[179, 12000] loss: 2.004
Final Summary:   loss: 1.986
[180,  2000] loss: 2.008
[180,  4000] loss: 1.973
[180,  6000] loss: 1.999
[180,  8000] loss: 2.129
[180, 10000] loss: 2.137
[180, 12000] loss: 2.047
Final Summary:   loss: 2.049
[181,  2000] loss: 2.040
[181,  4000] loss: 2.018
[181,  6000] loss: 2.015
[181,  8000] loss: 1.969
[181, 10000] loss: 1.961
[181, 12000] loss: 1.947
Final Summary:   loss: 1.990
Train Accuracy of the network: 51 %
Test Accuracy of the network: 51 %
[182,  2000] loss: 1.962
[182,  4000] loss: 1.992
[182,  6000] loss: 2.051
[182,  8000] loss: 1.962
[182, 10000] loss: 1.985
[182, 12000] loss: 1.955
Final Summary:   loss: 1.985
[183,  2000] loss: 1.951
[183,  4000] loss: 2.003
[183,  6000] loss: 1.974
[183,  8000] loss: 2.028
[183, 10000] loss: 2.013
[183, 12000] loss: 1.975
Final Summary:   loss: 1.990
[184,  2000] loss: 2.022
[184,  4000] loss: 2.093
[184,  6000] loss: 2.035
[184,  8000] loss: 2.091
[184, 10000] loss: 2.019
[184, 12000] loss: 2.049
Final Summary:   loss: 2.054
[185,  2000] loss: 2.188
[185,  4000] loss: 2.157
[185,  6000] loss: 2.107
[185,  8000] loss: 2.136
[185, 10000] loss: 2.102
[185, 12000] loss: 2.101
Final Summary:   loss: 2.131
[186,  2000] loss: 2.116
[186,  4000] loss: 2.129
[186,  6000] loss: 2.082
[186,  8000] loss: 2.132
[186, 10000] loss: 2.192
[186, 12000] loss: 2.200
Final Summary:   loss: 2.143
[187,  2000] loss: 2.126
[187,  4000] loss: 2.117
[187,  6000] loss: 2.127
[187,  8000] loss: 2.132
[187, 10000] loss: 2.135
[187, 12000] loss: 2.111
Final Summary:   loss: 2.124
[188,  2000] loss: 2.117
[188,  4000] loss: 2.106
[188,  6000] loss: 2.061
[188,  8000] loss: 2.056
[188, 10000] loss: 2.057
[188, 12000] loss: 2.080
Final Summary:   loss: 2.081
[189,  2000] loss: 2.121
[189,  4000] loss: 2.120
[189,  6000] loss: 2.083
[189,  8000] loss: 2.093
[189, 10000] loss: 2.084
[189, 12000] loss: 2.090
Final Summary:   loss: 2.099
[190,  2000] loss: 2.080
[190,  4000] loss: 2.097
[190,  6000] loss: 2.155
[190,  8000] loss: 2.172
[190, 10000] loss: 2.210
[190, 12000] loss: 2.236
Final Summary:   loss: 2.161
[191,  2000] loss: 2.205
[191,  4000] loss: 2.188
[191,  6000] loss: 2.177
[191,  8000] loss: 2.187
[191, 10000] loss: 2.207
[191, 12000] loss: 2.157
Final Summary:   loss: 2.185
[192,  2000] loss: 2.156
[192,  4000] loss: 2.176
[192,  6000] loss: 2.179
[192,  8000] loss: 2.183
[192, 10000] loss: 2.152
[192, 12000] loss: 2.093
Final Summary:   loss: 2.153
[193,  2000] loss: 2.075
[193,  4000] loss: 2.111
[193,  6000] loss: 2.103
[193,  8000] loss: 2.125
[193, 10000] loss: 2.126
[193, 12000] loss: 2.116
Final Summary:   loss: 2.110
[194,  2000] loss: 2.110
[194,  4000] loss: 2.115
[194,  6000] loss: 2.125
[194,  8000] loss: 2.122
[194, 10000] loss: 2.102
[194, 12000] loss: 2.113
Final Summary:   loss: 2.114
[195,  2000] loss: 2.081
[195,  4000] loss: 2.100
[195,  6000] loss: 2.077
[195,  8000] loss: 2.119
[195, 10000] loss: 2.127
[195, 12000] loss: 2.136
Final Summary:   loss: 2.109
[196,  2000] loss: 2.170
[196,  4000] loss: 2.163
[196,  6000] loss: 2.177
[196,  8000] loss: 2.255
[196, 10000] loss: 2.209
[196, 12000] loss: 2.185
Final Summary:   loss: 2.191
[197,  2000] loss: 2.130
[197,  4000] loss: 2.144
[197,  6000] loss: 2.124
[197,  8000] loss: 2.108
[197, 10000] loss: 2.107
[197, 12000] loss: 2.131
Final Summary:   loss: 2.126
[198,  2000] loss: 2.167
[198,  4000] loss: 2.119
[198,  6000] loss: 2.102
[198,  8000] loss: 2.130
[198, 10000] loss: 2.169
[198, 12000] loss: 2.222
Final Summary:   loss: 2.158
[199,  2000] loss: 2.285
[199,  4000] loss: 2.274
[199,  6000] loss: 2.265
[199,  8000] loss: 2.267
[199, 10000] loss: 2.250
[199, 12000] loss: 2.223
Final Summary:   loss: 2.259
[200,  2000] loss: 2.217
[200,  4000] loss: 2.216
[200,  6000] loss: 2.205
[200,  8000] loss: 2.196
[200, 10000] loss: 2.211
[200, 12000] loss: 2.179
Final Summary:   loss: 2.204
[201,  2000] loss: 2.180
[201,  4000] loss: 2.176
[201,  6000] loss: 2.182
[201,  8000] loss: 2.165
[201, 10000] loss: 2.186
[201, 12000] loss: 2.212
Final Summary:   loss: 2.185
Train Accuracy of the network: 24 %
Test Accuracy of the network: 26 %
[202,  2000] loss: 2.208
[202,  4000] loss: 2.228
[202,  6000] loss: 2.256
[202,  8000] loss: 2.257
[202, 10000] loss: 2.254
[202, 12000] loss: 2.261
Final Summary:   loss: 2.245

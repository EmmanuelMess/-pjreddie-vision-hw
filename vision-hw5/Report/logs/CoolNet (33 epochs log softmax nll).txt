[1,  2000] loss: 2.108
[1,  4000] loss: 1.892
[1,  6000] loss: 1.756
[1,  8000] loss: 1.648
[1, 10000] loss: 1.599
[1, 12000] loss: 1.534
Final Summary:   loss: 1.747
Train Accuracy of the network: 45 %
Test Accuracy of the network: 46 %
[2,  2000] loss: 1.505
[2,  4000] loss: 1.436
[2,  6000] loss: 1.422
[2,  8000] loss: 1.411
[2, 10000] loss: 1.380
[2, 12000] loss: 1.384
Final Summary:   loss: 1.419
[3,  2000] loss: 1.355
[3,  4000] loss: 1.305
[3,  6000] loss: 1.298
[3,  8000] loss: 1.271
[3, 10000] loss: 1.261
[3, 12000] loss: 1.249
Final Summary:   loss: 1.287
[4,  2000] loss: 1.217
[4,  4000] loss: 1.233
[4,  6000] loss: 1.222
[4,  8000] loss: 1.180
[4, 10000] loss: 1.189
[4, 12000] loss: 1.171
Final Summary:   loss: 1.201
[5,  2000] loss: 1.151
[5,  4000] loss: 1.126
[5,  6000] loss: 1.153
[5,  8000] loss: 1.132
[5, 10000] loss: 1.110
[5, 12000] loss: 1.104
Final Summary:   loss: 1.127
[6,  2000] loss: 1.077
[6,  4000] loss: 1.070
[6,  6000] loss: 1.093
[6,  8000] loss: 1.074
[6, 10000] loss: 1.063
[6, 12000] loss: 1.065
Final Summary:   loss: 1.074
[7,  2000] loss: 1.048
[7,  4000] loss: 1.045
[7,  6000] loss: 1.040
[7,  8000] loss: 1.023
[7, 10000] loss: 1.034
[7, 12000] loss: 1.021
Final Summary:   loss: 1.033
[8,  2000] loss: 0.985
[8,  4000] loss: 0.995
[8,  6000] loss: 1.005
[8,  8000] loss: 0.997
[8, 10000] loss: 0.999
[8, 12000] loss: 1.001
Final Summary:   loss: 0.999
[9,  2000] loss: 0.981
[9,  4000] loss: 0.971
[9,  6000] loss: 0.974
[9,  8000] loss: 0.976
[9, 10000] loss: 0.982
[9, 12000] loss: 0.942
Final Summary:   loss: 0.969
[10,  2000] loss: 0.957
[10,  4000] loss: 0.949
[10,  6000] loss: 0.927
[10,  8000] loss: 0.955
[10, 10000] loss: 0.958
[10, 12000] loss: 0.949
Final Summary:   loss: 0.948
[11,  2000] loss: 0.925
[11,  4000] loss: 0.942
[11,  6000] loss: 0.955
[11,  8000] loss: 0.928
[11, 10000] loss: 0.911
[11, 12000] loss: 0.910
Final Summary:   loss: 0.928
Train Accuracy of the network: 67 %
Test Accuracy of the network: 68 %
[1,  2000] loss: 0.909
[1,  4000] loss: 0.899
[1,  6000] loss: 0.907
[1,  8000] loss: 0.920
[1, 10000] loss: 0.914
[1, 12000] loss: 0.929
Final Summary:   loss: 0.912
Train Accuracy of the network: 69 %
Test Accuracy of the network: 69 %
[2,  2000] loss: 0.904
[2,  4000] loss: 0.910
[2,  6000] loss: 0.899
[2,  8000] loss: 0.895
[2, 10000] loss: 0.901
[2, 12000] loss: 0.886
Final Summary:   loss: 0.900
[3,  2000] loss: 0.883
[3,  4000] loss: 0.893
[3,  6000] loss: 0.878
[3,  8000] loss: 0.880
[3, 10000] loss: 0.870
[3, 12000] loss: 0.903
Final Summary:   loss: 0.884
[4,  2000] loss: 0.860
[4,  4000] loss: 0.861
[4,  6000] loss: 0.873
[4,  8000] loss: 0.910
[4, 10000] loss: 0.869
[4, 12000] loss: 0.866
Final Summary:   loss: 0.872
[5,  2000] loss: 0.878
[5,  4000] loss: 0.871
[5,  6000] loss: 0.847
[5,  8000] loss: 0.869
[5, 10000] loss: 0.849
[5, 12000] loss: 0.859
Final Summary:   loss: 0.863
[6,  2000] loss: 0.856
[6,  4000] loss: 0.838
[6,  6000] loss: 0.863
[6,  8000] loss: 0.835
[6, 10000] loss: 0.866
[6, 12000] loss: 0.839
Final Summary:   loss: 0.850
[7,  2000] loss: 0.831
[7,  4000] loss: 0.861
[7,  6000] loss: 0.832
[7,  8000] loss: 0.839
[7, 10000] loss: 0.847
[7, 12000] loss: 0.845
Final Summary:   loss: 0.844
[8,  2000] loss: 0.852
[8,  4000] loss: 0.827
[8,  6000] loss: 0.828
[8,  8000] loss: 0.838
[8, 10000] loss: 0.838
[8, 12000] loss: 0.827
Final Summary:   loss: 0.835
[9,  2000] loss: 0.812
[9,  4000] loss: 0.809
[9,  6000] loss: 0.830
[9,  8000] loss: 0.837
[9, 10000] loss: 0.842
[9, 12000] loss: 0.821
Final Summary:   loss: 0.825
[10,  2000] loss: 0.818
[10,  4000] loss: 0.809
[10,  6000] loss: 0.833
[10,  8000] loss: 0.814
[10, 10000] loss: 0.826
[10, 12000] loss: 0.803
Final Summary:   loss: 0.818
[11,  2000] loss: 0.815
[11,  4000] loss: 0.820
[11,  6000] loss: 0.803
[11,  8000] loss: 0.798
[11, 10000] loss: 0.809
[11, 12000] loss: 0.820
Final Summary:   loss: 0.812
Train Accuracy of the network: 72 %
Test Accuracy of the network: 70 %
[1,  2000] loss: 0.807
[1,  4000] loss: 0.815
[1,  6000] loss: 0.800
[1,  8000] loss: 0.825
[1, 10000] loss: 0.811
[1, 12000] loss: 0.815
Final Summary:   loss: 0.811
Train Accuracy of the network: 72 %
Test Accuracy of the network: 71 %
[2,  2000] loss: 0.793
[2,  4000] loss: 0.802
[2,  6000] loss: 0.787
[2,  8000] loss: 0.789
[2, 10000] loss: 0.808
[2, 12000] loss: 0.812
Final Summary:   loss: 0.800
[3,  2000] loss: 0.795
[3,  4000] loss: 0.787
[3,  6000] loss: 0.785
[3,  8000] loss: 0.809
[3, 10000] loss: 0.795
[3, 12000] loss: 0.802
Final Summary:   loss: 0.795
[4,  2000] loss: 0.791
[4,  4000] loss: 0.790
[4,  6000] loss: 0.797
[4,  8000] loss: 0.790
[4, 10000] loss: 0.795
[4, 12000] loss: 0.766
Final Summary:   loss: 0.789
[5,  2000] loss: 0.767
[5,  4000] loss: 0.782
[5,  6000] loss: 0.786
[5,  8000] loss: 0.795
[5, 10000] loss: 0.801
[5, 12000] loss: 0.797
Final Summary:   loss: 0.787
[6,  2000] loss: 0.794
[6,  4000] loss: 0.790
[6,  6000] loss: 0.787
[6,  8000] loss: 0.777
[6, 10000] loss: 0.767
[6, 12000] loss: 0.790
Final Summary:   loss: 0.784
[7,  2000] loss: 0.762
[7,  4000] loss: 0.793
[7,  6000] loss: 0.775
[7,  8000] loss: 0.772
[7, 10000] loss: 0.775
[7, 12000] loss: 0.774
Final Summary:   loss: 0.776
[8,  2000] loss: 0.743
[8,  4000] loss: 0.775
[8,  6000] loss: 0.771
[8,  8000] loss: 0.771
[8, 10000] loss: 0.795
[8, 12000] loss: 0.784
Final Summary:   loss: 0.774
[9,  2000] loss: 0.783
[9,  4000] loss: 0.760
[9,  6000] loss: 0.771
[9,  8000] loss: 0.764
[9, 10000] loss: 0.768
[9, 12000] loss: 0.764
Final Summary:   loss: 0.769
[10,  2000] loss: 0.767
[10,  4000] loss: 0.770
[10,  6000] loss: 0.744
[10,  8000] loss: 0.762
[10, 10000] loss: 0.767
[10, 12000] loss: 0.755
Final Summary:   loss: 0.763
[11,  2000] loss: 0.744
[11,  4000] loss: 0.744
[11,  6000] loss: 0.756
[11,  8000] loss: 0.767
[11, 10000] loss: 0.773
[11, 12000] loss: 0.768
Final Summary:   loss: 0.760
Train Accuracy of the network: 73 %
Test Accuracy of the network: 72 %
